{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark for Your Notebook\n",
    "* This examples uses the local Spark Master `--master local[1]`\n",
    "* In production, you would use the PipelineIO Spark Master `--master spark://apachespark-master-2-1-0:7077`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--master local[1] --conf spark.cores.max=1 --conf spark.executor.memory=512m --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1 --jars /root/lib/jpmml-sparkml-package-1.0-SNAPSHOT.jar --py-files /root/lib/jpmml.py pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "master = '--master local[1]'\n",
    "#master = '--master spark://apachespark-master-2-1-0:7077'\n",
    "conf = '--conf spark.cores.max=1 --conf spark.executor.memory=512m'\n",
    "packages = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'\n",
    "jars = '--jars /root/lib/jpmml-sparkml-package-1.0-SNAPSHOT.jar'\n",
    "py_files = '--py-files /root/lib/jpmml.py'\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = master \\\n",
    "  + ' ' + conf \\\n",
    "  + ' ' + packages \\\n",
    "  + ' ' + jars \\\n",
    "  + ' ' + py_files \\\n",
    "  + ' ' + 'pyspark-shell'\n",
    "\n",
    "print(os.environ['PYSPARK_SUBMIT_ARGS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spark Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session\n",
    "This may take a minute or two.  Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkSession = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Training Data into Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from Public S3 Bucket\n",
    "* AWS credentials are not needed.\n",
    "* We're asking Spark to infer the schema\n",
    "* The data has a header\n",
    "* Using `bzip2` because it's a splittable compression file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=39, workclass='State-gov', education='Bachelors', education_num=13, marital_status='Never-married', occupation='Adm-clerical', relationship='Not-in-family', race='White', sex='Male', capital_gain=2174, capital_loss=0, hours_per_week=40, native_country='United-States', income='<=50K')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sparkSession.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\").option(\"header\", \"true\") \\\n",
    "  .load(\"s3a://datapalooza/R/census.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1f59179bebff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering\n",
    "Already engineered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula = RFormula(formula = \"income ~ .\")\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "pipeline = Pipeline(stages = [formula, classifier])\n",
    "\n",
    "pipelineModel = pipeline.fit(data)\n",
    "\n",
    "print(pipelineModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Export the Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from jpmml import toPMMLBytes\n",
    "\n",
    "model = toPMMLBytes(spark, training_dataset, pipeline_model)\n",
    "\n",
    "with open('census.model', 'wb') as fh:\n",
    "    fh.write(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:  Deploy the Pipeline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Option 1 of 2:  Use PipelineIO Command Line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -q pio-cli==0.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure CLI for Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pio init-model --model-server-url=http://prediction-jvm.demo.pipeline.io \\\n",
    "    --model-type=pmml --model-namespace=default --model-name=census-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pio deploy --model-version=v0 census.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Option 2 of 2:  REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "deploy_url = 'http://prediction-jvm.demo.pipeline.io/api/v1/model/deploy/pmml/default/census-rest/v0'\n",
    "\n",
    "files = {'file': open('census.model', 'rb')}\n",
    "\n",
    "response = requests.post(deploy_url, files=files)\n",
    "\n",
    "print(\"Success! %s\" % response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:  Predict With Deployed Pipeline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Prediction Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"age\":39,\n",
    "        \"workclass\":\"State-gov\",\n",
    "        \"education\":\"Bachelors\",\n",
    "        \"education_num\":13,\n",
    "        \"marital_status\":\"Never-married\",\n",
    "        \"occupation\":\"Adm-clerical\",\n",
    "        \"relationship\":\"Not-in-family\",\n",
    "        \"race\":\"White\",\n",
    "        \"sex\":\"Male\",\n",
    "        \"capital_gain\":2174,\n",
    "        \"capital_loss\":0,\n",
    "        \"hours_per_week\":40,\n",
    "        \"native_country\":\"United-States\"}\n",
    "\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "with open('census-predict-inputs.json', 'wt') as fh:\n",
    "    fh.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pio predict --model-version=v0 \\\n",
    "            --model-input-filename=census-predict-inputs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Note:  You may need to run this twice.\n",
    "#        A fallback will trigger the first time. (Bug)\n",
    "#predict_url = 'http://prediction-jvm.demo.pipeline.io/api/v1/model/predict/pmml/default/census-cli/v0'\n",
    "\n",
    "predict_url = 'http://prediction-jvm.demo.pipeline.io/api/v1/model/predict/pmml/default/pmml_census/v0'\n",
    "\n",
    "headers = {'content-type': 'application/json'}\n",
    "\n",
    "response = requests.post(predict_url, \n",
    "                         data=json_data, \n",
    "                         headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7:  Monitor Model Servers through Dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallbacks and Circuit Breaker [Dashboard](http://hystrix.demo.pipeline.io/hystrix-dashboard/monitor/monitor.html?streams=%5B%7B%22name%22%3A%22Model%20Servers%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%5D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<iframe width=800 height=600 src=\"http://hystrix.demo.pipeline.io/hystrix-dashboard/monitor/monitor.html?streams=%5B%7B%22name%22%3A%22Model%20Servers%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%5D\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafana Prediction Metrics [Dashboard](http://grafana.demo.pipeline.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<iframe width=800 height=600 src=\"http://grafana.demo.pipeline.io\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
