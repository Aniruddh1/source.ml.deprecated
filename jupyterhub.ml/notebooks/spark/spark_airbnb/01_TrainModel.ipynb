{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark for Your Notebook\n",
    "* This examples uses the local Spark Master `--master local[1]`\n",
    "* In production, you would use the PipelineIO Spark Master `--master spark://apachespark-master-2-1-0:7077`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "master = '--master local[1]'\n",
    "#master = '--master spark://apachespark-master-2-1-0:7077'\n",
    "conf = '--conf spark.cores.max=1 --conf spark.executor.memory=512m'\n",
    "packages = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'\n",
    "jars = '--jars /root/lib/jpmml-sparkml-package-1.0-SNAPSHOT.jar'\n",
    "py_files = '--py-files /root/lib/jpmml.py'\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = master \\\n",
    "  + ' ' + conf \\\n",
    "  + ' ' + packages \\\n",
    "  + ' ' + jars \\\n",
    "  + ' ' + py_files \\\n",
    "  + ' ' + 'pyspark-shell'\n",
    "\n",
    "print(os.environ['PYSPARK_SUBMIT_ARGS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spark Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark for Your Notebook\n",
    "* You may need to Reconnect and/or Restart the Kernel to pick up changes.\n",
    "* This examples uses the local Spark Master `--master local[1]`\n",
    "* In production, you would use the PipelineIO Spark Master `--master spark://apachespark-master-2-1-0:7077`zzaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session\n",
    "This may take a minute or two.  Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from Public S3 Bucket\n",
    "* AWS credentials are not needed.\n",
    "* We're asking Spark to infer the schema\n",
    "* The data has a header\n",
    "* Using `bzip2` because it's a splittable compression file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"s3a://datapalooza/airbnb/airbnb.csv.bz2\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean, Filter, and Summarize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(\"price >= 50 AND price <= 750 AND bathrooms > 0.0 AND bedrooms is not null\")\n",
    "\n",
    "df_filtered.registerTempTable(\"df_filtered\")\n",
    "\n",
    "df_final = spark_session.sql(\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        cast(price as double) as price,\n",
    "        cast(bathrooms as double) as bathrooms,\n",
    "        cast(bedrooms as double) as bedrooms,\n",
    "        room_type,\n",
    "        host_is_super_host,\n",
    "        cancellation_policy,\n",
    "        cast(case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as double) as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        cast(case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as double) as number_of_reviews,\n",
    "        cast(case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as double) as extra_people,\n",
    "        instant_bookable,\n",
    "        cast(case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as double) as cleaning_fee,\n",
    "        cast(case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as double) as review_scores_rating,\n",
    "        cast(case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as double) as square_feet\n",
    "    from df_filtered\n",
    "\"\"\").persist()\n",
    "\n",
    "df_final.registerTempTable(\"df_final\")\n",
    "\n",
    "df_final.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular cities\n",
    "\n",
    "spark_session.sql(\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as ct,\n",
    "        avg(price) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df_final\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most expensive popular cities\n",
    "\n",
    "spark_session.sql(\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as ct,\n",
    "        avg(price) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df_final\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"ct > 25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Continous and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_features = [\"bathrooms\", \\\n",
    "                       \"bedrooms\", \\\n",
    "                       \"security_deposit\", \\\n",
    "                       \"cleaning_fee\", \\\n",
    "                       \"extra_people\", \\\n",
    "                       \"number_of_reviews\", \\\n",
    "                       \"square_feet\", \\\n",
    "                       \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\", \\\n",
    "                        \"host_is_super_host\", \\\n",
    "                        \"cancellation_policy\", \\\n",
    "                        \"instant_bookable\", \\\n",
    "                        \"state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_feature_assembler = VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\n",
    "\n",
    "continuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\", \\\n",
    "                                           withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_feature_indexers = [StringIndexer(inputCol=x, \\\n",
    "                                              outputCol=\"{}_index\".format(x)) \\\n",
    "                                for x in categorical_features]\n",
    "\n",
    "categorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), \\\n",
    "                                                      outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) \\\n",
    "                                        for x in categorical_feature_indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols_lr = [x.getOutputCol() \\\n",
    "                   for x in categorical_feature_one_hot_encoders]\n",
    "feature_cols_lr.append(\"scaled_continuous_features\")\n",
    "\n",
    "feature_assembler_lr = VectorAssembler(inputCols=feature_cols_lr, \\\n",
    "                                       outputCol=\"features_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LR Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(featuresCol=\"features_lr\", \\\n",
    "                                     labelCol=\"price\", \\\n",
    "                                     predictionCol=\"price_prediction\", \\\n",
    "                                     maxIter=10, \\\n",
    "                                     regParam=0.3, \\\n",
    "                                     elasticNetParam=0.8)\n",
    "\n",
    "estimators_lr = \\\n",
    "  [continuous_feature_assembler, continuous_feature_scaler] \\\n",
    "  + categorical_feature_indexers + categorical_feature_one_hot_encoders \\\n",
    "  + [feature_assembler_lr] + [linear_regression]\n",
    "\n",
    "pipeline = Pipeline(stages=estimators_lr)\n",
    "\n",
    "pipeline_model = pipeline.fit(df_final)\n",
    "\n",
    "print(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Spark ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jpmml import toPMMLBytes\n",
    "\n",
    "model = toPMMLBytes(spark_session, df_final, pipeline_model)\n",
    "with open('model.spark', 'wb') as fh:\n",
    "    fh.write(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l model.spark"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
